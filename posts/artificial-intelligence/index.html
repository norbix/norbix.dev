<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><script>(function(){const e=localStorage.getItem("theme"),t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches;e==="dark"||!e&&t?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark"),window.setTheme=function(e){localStorage.setItem("theme",e),document.documentElement.classList.toggle("dark",e==="dark")}})()</script><title>Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning | norbix.dev
</title><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning | norbix.dev - The log of my journey through code & software systems architecture</title>
<meta name=keywords content="artificial-intelligence,machine-learning,deep-learning,ai-vs-ml"><meta name=description content="Understand the difference between Artificial Intelligence, Machine Learning, and Deep Learning. Learn how these concepts fit together and power modern software systems."><meta name=author content><link rel=canonical href=https://norbix.dev/posts/artificial-intelligence/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://norbix.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://norbix.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://norbix.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://norbix.dev/apple-touch-icon.png><link rel=mask-icon href=https://norbix.dev/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://norbix.dev/posts/artificial-intelligence/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=canonical href=https://norbix.dev/posts/artificial-intelligence/><meta property="og:title" content="Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning"><meta property="og:url" content="https://norbix.dev/posts/artificial-intelligence/"><meta property="og:image" content="https://norbix.dev/ai-banner.jpg"><meta property="og:type" content="article"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning"><meta name=twitter:image content="https://norbix.dev/ai-banner.jpg"><script data-goatcounter=https://norbix.goatcounter.com/count async src=//gc.zgo.at/count.js></script><link rel=alternate type=application/rss+xml title="RSS Feed for norbix.dev" href=/index.xml><script src=https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js defer></script><script>document.addEventListener("DOMContentLoaded",function(){window.mermaid&&(document.querySelectorAll("code.language-mermaid").forEach(function(e){var n=e.parentElement,t=document.createElement("div");t.className="mermaid",t.textContent=e.textContent,n.replaceWith(t)}),mermaid.initialize({startOnLoad:!0}))})</script><link href=https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/css/lightbox.min.css rel=stylesheet><script src=https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.3/js/lightbox.min.js defer></script><meta property="og:title" content="Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning"><meta property="og:description" content="Understand the difference between Artificial Intelligence, Machine Learning, and Deep Learning. Learn how these concepts fit together and power modern software systems."><meta property="og:type" content="article"><meta property="og:url" content="https://norbix.dev/posts/artificial-intelligence/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-04T18:00:00+02:00"><meta property="article:modified_time" content="2025-09-04T18:00:00+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning"><meta name=twitter:description content="Understand the difference between Artificial Intelligence, Machine Learning, and Deep Learning. Learn how these concepts fit together and power modern software systems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://norbix.dev/posts/"},{"@type":"ListItem","position":2,"name":"Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning","item":"https://norbix.dev/posts/artificial-intelligence/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning","name":"Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning","description":"Understand the difference between Artificial Intelligence, Machine Learning, and Deep Learning. Learn how these concepts fit together and power modern software systems.","keywords":["artificial-intelligence","machine-learning","deep-learning","ai-vs-ml"],"articleBody":" ‚ÄúArtificial Intelligence isn‚Äôt about replacing humans. It‚Äôs about amplifying human potential.‚Äù\nArtificial Intelligence (AI) is one of the most transformative forces in technology today. From recommendation engines on Netflix to self-driving cars and generative models like ChatGPT, AI is shaping how we work, live, and create.\nBut AI is often misunderstood. Is it the same as machine learning? Where does deep learning fit? Let‚Äôs break it down.\nüìú A Brief History of AI 1950s ‚Äì Alan Turing proposes the Turing Test. Early symbolic AI emerges. 1980s‚Äì1990s ‚Äì Expert systems and rule-based knowledge engines dominate. 2000s ‚Äì Rise of statistical machine learning thanks to bigger datasets. 2010s ‚Äì Deep learning revolution with neural networks and GPUs. 2020s ‚Äì Generative AI (ChatGPT, Claude, Gemini) makes AI mainstream. üîπ Tip: AI has decades of research behind it ‚Äî what feels ‚Äúnew‚Äù is the scale and accessibility today.\nüß† Artificial Intelligence: The Big Picture Artificial Intelligence (AI) is the broad field focused on creating systems that mimic human intelligence.\nExamples include:\nRule-based systems (e.g., chess engines from the 1980s) Natural language processing (chatbots, translators) Computer vision (face recognition, object detection) Robotics and autonomous systems AI doesn‚Äôt always require learning. A simple rule-based expert system is AI, even if it doesn‚Äôt adapt over time.\nüîπ Tip: Think of AI as the goal ‚Äî making machines ‚Äúsmart.‚Äù\nü§î How ChatGPT Works Behind the Scenes One of today‚Äôs most visible applications of AI is ChatGPT, a large language model built using deep learning. Here‚Äôs how it works at a high level:\nTraining on huge datasets ‚Äì Learns statistical patterns from books, code, and the web. Neural network architecture ‚Äì Uses Transformers to capture relationships between words. Token prediction ‚Äì Predicts the most likely next word (token) in a sequence. Fine-tuning \u0026 RLHF ‚Äì Reinforcement learning from human feedback aligns responses. Inference ‚Äì At runtime, your input is converted into tokens, processed through billions of neural weights, and output as natural language. üîπ Tip: ChatGPT doesn‚Äôt ‚Äúunderstand‚Äù like a human. It‚Äôs a probabilistic pattern-matching engine.\nüîÑ Other AI Models Competing with ChatGPT The market is full of competitors, each with different strengths:\nClaude (Anthropic): Long context, reasoning, ethical design. Google Gemini: Multimodal (text, image, audio, video). xAI Grok: Multimodal with real-time search, integrated in X/Tesla. Perplexity: AI + live web search with citations. Microsoft Copilot: Embedded in Office/Teams with GPT-4 Turbo. Meta AI (LLaMA): Social/media apps, open research focus. DeepSeek (China): Efficiency-driven, strong benchmarks. Mistral AI (EU): Open-source, long context, developer-friendly. Moonshot AI (China): Large trillion-parameter ‚ÄúKimi‚Äù models. YandexGPT: Russian-focused business integrations. Model Strengths Best For Claude Long context, reasoning Research \u0026 enterprise workflows Gemini Multimodal, Google ecosystem Cross-media AI Grok Real-time retrieval, reasoning Social/voice-first apps Perplexity Citations, fact-checking Research and knowledge tasks Copilot Deep MS integration Productivity workflows Meta AI Social media ecosystem Chat \u0026 consumer interaction DeepSeek Energy-efficient reasoning Scale-sensitive applications Mistral Open-source, flexible Developer tooling \u0026 customization Moonshot AI Massive models, multimodal Cutting-edge innovation YandexGPT Localized enterprise AI Russian-language businesses üîπ Tip: Pick your AI model based on ecosystem fit (Google, Microsoft, Meta), task type (research vs creative), and control (open vs closed source).\nüìä Machine Learning: Learning from Data Machine Learning (ML) is a subset of AI. Instead of hard-coding rules, ML algorithms learn from data and improve with exposure.\nApplications: spam filters, predictive maintenance, fraud detection, recommendations.\nMethods: regression, decision trees, clustering, reinforcement learning.\nüîπ Tip: ML is the toolbox that powers modern AI.\nWhat is a Model in Machine Learning? A model in machine learning is basically a mathematical function that:\nTakes inputs (features, e.g. hours_studied).\nProduces an output (prediction, e.g. expected score).\nLearns the relationship between inputs and outputs by looking at examples in data.\nThe logic or formula type the machine learning system is trying to learn.\nParameters The specific values in that logic that the training process discovers.\nImagine you want to guess a student‚Äôs exam score based on how many hours they studied.\nYou collect data:\ncreate table\nhours_studied score 1 50 2 60 3 70 A model could be as simple as the rule:\n1 score = 10 * hours_studied + 40 Model = ‚Äúlinear relationship between hours and score‚Äù Parameters = slope = 10 and intercept = 40 Analogy Model = the shape of the recipe (e.g., ‚Äúbake a cake‚Äù).\nThat‚Äôs just a formula ‚Äî and that‚Äôs what a model is. Parameters = the exact ingredient amounts (200g flour, 2 eggs, 100g sugar).\nTraining = adjusting ingredient amounts until the cake tastes right.\nA model is the logic found.\nParameters are the variables (numbers) that make that logic concrete\nHow it works? You don‚Äôt hand-code the rules (like if hours \u003e 5 then good score).\nInstead, you give the model examples (inputs + correct outputs).\nThe training process adjusts internal parameters until the model finds rules that best fit the data.\nSo yes ‚Äî in a way, the model is ‚Äúcreating its own logic‚Äù.\nYou don‚Äôt write:\n1 2 def predict(hours): return 10 * hours + 40 The model discovers that rule by itself, because that line best fits the data.\nTypes of models Linear Regression ‚Üí learns straight-line formulas. Linear Regression ‚Üí predicts continuous values with straight-line formulas.\nLogistic Regression ‚Üí predicts probabilities/classes with a linear boundary.\nLinear SVM ‚Üí finds a straight hyperplane to separate classes.\nüëâ Good for: simple, linearly separable problems.\nüëâ Limitation: can‚Äôt capture curves, waves, or complex shapes ‚Üí risk of underfitting.\nDecision Trees ‚Üí learns rules like ‚Äúif amount \u003e 1000 then fraud.‚Äù Decision Trees ‚Üí split data with simple rules (if amount \u003e 1000 ‚Üí fraud).\nRandom Forests ‚Üí many trees combined ‚Üí better accuracy, less overfitting.\nGradient Boosted Trees (XGBoost, LightGBM, CatBoost) ‚Üí trees built in sequence to fix each other‚Äôs mistakes.\nüëâ Good for: tabular data (transactions, customer info).\nüëâ Strength: handles non-linear patterns, interactions.\nNeural Networks ‚Üí complex layered functions that can learn images, text, etc. Simple feed-forward networks (MLPs) ‚Üí capture non-linear patterns.\nCNNs (Convolutional Neural Networks) ‚Üí great for images.\nRNNs / LSTMs / Transformers ‚Üí great for sequences, text, time series.\nüëâ Good for: complex patterns (vision, NLP, speech).\nüëâ Strength: very flexible, can approximate almost any function.\nüëâ Limitation: need lots of data + compute.\nüëâ In short: A model is a trained function that tries to map inputs ‚Üí outputs based on patterns it finds in the data.\nProbabilistic Models Na√Øve Bayes ‚Üí simple, based on probability rules.\nHidden Markov Models ‚Üí sequential, time-series modeling.\nüëâ Good for: small datasets, text classification, spam detection.\nClustering / Unsupervised Models K-Means ‚Üí groups similar points.\nHierarchical Clustering ‚Üí builds a tree of clusters.\nDBSCAN ‚Üí finds clusters of arbitrary shapes.\nüëâ Good for: when you don‚Äôt have labels (unsupervised learning).\n‚ö° How to choose? If the pattern is simple \u0026 linear ‚Üí linear regression / logistic regression.\nIf the data has rules \u0026 thresholds ‚Üí tree-based models.\nIf the problem is very complex (images, text, audio, high-dimensional data) ‚Üí neural networks.\nIf you have no labels ‚Üí clustering methods.\n‚ö†Ô∏è Common ML Challenges: Imbalanced Data \u0026 Generalization 1. Imbalanced Classes üö® Problem Happens when one class dominates the dataset.\nExample: Fraud detection ‚Üí 99% ‚Äúlegit‚Äù vs 1% ‚Äúfraud‚Äù.\nIf you train a classifier, it might always predict the majority class and still get 99% accuracy.\nExample Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"dataset\": [ { \"transaction_id\": \"tx001\", \"amount\": 120.50, \"location\": \"NY\", \"label\": \"legit\" }, { \"transaction_id\": \"tx002\", \"amount\": 80.00, \"location\": \"CA\", \"label\": \"legit\" }, { \"transaction_id\": \"tx003\", \"amount\": 75.00, \"location\": \"TX\", \"label\": \"legit\" }, { \"transaction_id\": \"tx004\", \"amount\": 200.00, \"location\": \"NY\", \"label\": \"legit\" }, { \"transaction_id\": \"tx005\", \"amount\": 950.00, \"location\": \"FL\", \"label\": \"legit\" }, { \"transaction_id\": \"tx006\", \"amount\": 20.00, \"location\": \"WA\", \"label\": \"legit\" }, { \"transaction_id\": \"tx007\", \"amount\": 500.00, \"location\": \"IL\", \"label\": \"legit\" }, { \"transaction_id\": \"tx008\", \"amount\": 50.00, \"location\": \"NV\", \"label\": \"legit\" }, { \"transaction_id\": \"tx009\", \"amount\": 100.00, \"location\": \"NY\", \"label\": \"legit\" }, { \"transaction_id\": \"tx010\", \"amount\": 5000.00,\"location\": \"CA\", \"label\": \"fraud\" } ] } üõ†Ô∏è Solutions Resampling the dataset\nOversampling minority class (e.g., SMOTE ‚Äì Synthetic Minority Oversampling Technique).\nUndersampling majority class to balance the distribution.\nAdjusting class weights\nPenalize mistakes on the minority class more heavily (supported in many ML frameworks). Choosing the right metrics\nAccuracy is misleading. Better: Precision, Recall, F1-score, ROC-AUC, PR-AUC.\nFor fraud, often maximize recall (catch as many frauds as possible) at the expense of some false positives.\nüëâ Key interview takeaway: ‚ÄúWith imbalanced data, I focus on resampling, adjusting class weights, and using metrics beyond accuracy, like precision, recall, and ROC-AUC.‚Äù\n2. Overfitting üö® Problem Model learns too much from training data (including noise and quirks).\nGreat on training set, bad on unseen/test data.\nüõ†Ô∏è Symptoms High training accuracy, low validation/test accuracy.\nLoss continues dropping on training, but rises on validation (classic overfitting curve).\nExample Dataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"training_set\": [ { \"student_id\": \"s001\", \"hours_studied\": 1, \"score\": 50 }, { \"student_id\": \"s002\", \"hours_studied\": 2, \"score\": 60 }, { \"student_id\": \"s003\", \"hours_studied\": 3, \"score\": 70 }, { \"student_id\": \"s004\", \"hours_studied\": 4, \"score\": 65 }, { \"student_id\": \"s005\", \"hours_studied\": 5, \"score\": 80 } ], \"test_set\": [ { \"student_id\": \"s101\", \"hours_studied\": 6, \"score\": 85 }, { \"student_id\": \"s102\", \"hours_studied\": 7, \"score\": 90 }, { \"student_id\": \"s103\", \"hours_studied\": 8, \"score\": 95 } ] } What‚Äôs happening here?\nIn the training data, look at ‚Äúhours_studied‚Äù: 4.\nInstead of following the trend (50 ‚Üí 60 ‚Üí 70 ‚Üí 80‚Ä¶), the score drops to 65.\nThis is noise.\nA complex model might ‚Äúmemorize‚Äù that drop and think:\n‚ÄúStudying 4 hours actually lowers your score.‚Äù On the test set, where the true trend continues upward (6 ‚Üí 85, 7 ‚Üí 90, 8 ‚Üí 95), the model makes wrong predictions because it learned the noise.\nThat‚Äôs overfitting:\n‚úÖ Training accuracy = high (because it memorized everything).\n‚ùå Test accuracy = low (because it didn‚Äôt generalize the real rule).\nüõ†Ô∏è Solutions Regularization: L1 (sparsity), L2 (weight decay).\nDropout (turning off random neurons during training).\nEarly stopping (halt training when validation loss worsens).\nSimpler model (reduce number of parameters).\nMore data / data augmentation (especially in image tasks).\nüëâ Key interview takeaway: ‚ÄúOverfitting is when the model memorizes instead of generalizing. I fight it with regularization, dropout, early stopping, and more data.‚Äù\n3. Underfitting üö® Problem Model is too simple to capture the underlying patterns.\nPoor performance on both training and test sets.\nüõ†Ô∏è Symptoms Both training and validation accuracy are low.\nLoss is high and doesn‚Äôt improve.\nExample 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"dataset\": [ { \"x\": 1, \"y\": 1 }, { \"x\": 2, \"y\": 4 }, { \"x\": 3, \"y\": 9 }, { \"x\": 4, \"y\": 16 }, { \"x\": 5, \"y\": 25 }, { \"x\": 6, \"y\": 36 }, { \"x\": 7, \"y\": 49 }, { \"x\": 8, \"y\": 64 }, { \"x\": 9, \"y\": 81 }, { \"x\": 10, \"y\": 100 } ] } Underfitting = the model is too simple, so it fails to learn even the obvious pattern.\nExample: the real relationship is non-linear (curved), but the model tries to force a straight line.\nWhy this shows underfitting The true pattern is quadratic:\n1 y=x2 If you force a linear model (straight line, e.g. y = ax + b), it won‚Äôt fit well:\nAt small x, it predicts too high.\nAt large x, it predicts too low.\nResult:\n‚ùå Training accuracy = low.\n‚ùå Test accuracy = low.\nThat‚Äôs underfitting ‚Äî the model is too simple for the data‚Äôs complexity.\n‚ö° Rule of thumb If you know the pattern is quadratic ‚Üí Polynomial Regression is the cleanest choice.\nIf you suspect it could be more complex ‚Üí go with trees or a small neural net.\nüõ†Ô∏è Solutions Use a more complex model (more layers, deeper tree, etc.).\nTrain longer (more epochs, better learning rate).\nFeature engineering (add informative features).\nReduce regularization (too strong regularization may cause underfitting).\nüëâ Key interview takeaway:‚ÄúUnderfitting is when the model is too simple. To fix it, I increase model complexity, add better features, or train longer.‚Äù\nüß† Connecting the Dots Imbalanced classes: The data distribution is skewed ‚Üí accuracy is misleading.\nOverfitting: The model is too complex ‚Üí memorizes instead of generalizing.\nUnderfitting: The model is too simple ‚Üí fails to learn meaningful patterns.\nü§ñ Deep Learning: The Neural Revolution ü§ñ Deep Learning: The Neural Revolution Deep Learning (DL) is a subset of ML that relies on artificial neural networks (ANNs) with many layers. These layers allow the model to learn increasingly complex representations of data ‚Äî from edges in an image to entire concepts like ‚Äúcat‚Äù or ‚Äúcar.‚Äù\nüß© What Are Neural Networks? Inspired by biology ‚Äì loosely modeled after neurons in the human brain.\nStructure ‚Äì input layer (data), hidden layers (transformations), output layer (prediction).\nConnections ‚Äì each neuron has weights and biases, adjusted during training.\nActivation functions ‚Äì nonlinear transformations (ReLU, sigmoid, tanh, softmax) that let networks learn complex relationships.\nüëâ Without activation functions, a neural network would just be a fancy linear regression.\nüîÑ How Neural Networks Learn The training process follows a loop:\nForward pass ‚Äì input flows through layers, producing an output.\nLoss function ‚Äì measures how far the prediction is from the correct answer.\nBackward pass (backpropagation) ‚Äì calculates gradients of the loss with respect to weights.\nOptimization (gradient descent) ‚Äì updates weights to reduce error.\nThis cycle repeats thousands or millions of times until the network converges on good parameters.\nüèóÔ∏è Types of Neural Networks Feedforward Networks (MLP) ‚Äì simplest form, fully connected layers.\nConvolutional Neural Networks (CNNs) ‚Äì specialized for images and spatial data (e.g., object detection, face recognition).\nRecurrent Neural Networks (RNNs) ‚Äì designed for sequences (e.g., speech, text, time-series).\nTransformers ‚Äì modern architecture for language, vision, and multimodal tasks (powering GPT, Gemini, Claude).\n‚ö° Why Deep Learning Works So Well Learns hierarchical features automatically (no manual feature engineering).\nScales with big data and powerful hardware (GPUs/TPUs).\nExcels at unstructured data: images, audio, text.\nüåç Real-World Applications Image recognition ‚Äì self-driving cars, medical imaging.\nSpeech recognition ‚Äì voice assistants, transcription.\nNatural language processing ‚Äì chatbots, translation, sentiment analysis.\nGenerative AI ‚Äì LLMs (ChatGPT, Claude), diffusion models (Stable Diffusion, MidJourney).\nüîπ Tip: Deep learning is what made AI feel magical ‚Äî moving from ‚Äúmachines that calculate‚Äù to ‚Äúmachines that see, listen, and talk.‚Äù\nSimple diagram of a feedforward neural network with one hidden layer Input layer: features (e.g., pixels, words, measurements). Hidden layer: neurons transform inputs using weights + activation functions. Output layer: final prediction (classification, regression, etc.). graph LR subgraph Input[\"Input Layer\"] I1[\"x‚ÇÅ\"] I2[\"x‚ÇÇ\"] I3[\"x‚ÇÉ\"] end subgraph Hidden[\"Hidden Layer (Neurons)\"] H1[\"h‚ÇÅ\"] H2[\"h‚ÇÇ\"] H3[\"h‚ÇÉ\"] end subgraph Output[\"Output Layer\"] O1[\"≈∑ (prediction)\"] end I1 --\u003e H1 I1 --\u003e H2 I1 --\u003e H3 I2 --\u003e H1 I2 --\u003e H2 I2 --\u003e H3 I3 --\u003e H1 I3 --\u003e H2 I3 --\u003e H3 H1 --\u003e O1 H2 --\u003e O1 H3 --\u003e O1 üõ†Ô∏è Key AI Techniques Beyond ML AI also includes:\nSearch algorithms (A*, minimax in games) Planning systems (robotics, logistics scheduling) Knowledge graphs \u0026 reasoning (semantic web, ontologies) Rule-based expert systems (if-else driven logic engines) üëâ Not all AI is ML ‚Äî classic approaches still power many systems.\n‚öñÔ∏è AI vs. ML vs. DL: A Mental Model One of the biggest sources of confusion in tech discussions is the relationship between Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL). The simplest way to think about it is as nested circles:\nAI (Artificial Intelligence) ‚Äì the broadest concept.\nThe goal: make machines simulate human intelligence.\nIncludes both learning systems and rule-based systems.\nExamples: expert systems, knowledge graphs, search algorithms, game-playing bots, natural language processing, robotics.\nüëâ AI is the what ‚Äî the ambition of making machines act smart.\nML (Machine Learning) ‚Äì a subset of AI.\nThe method: algorithms that learn patterns from data instead of relying on hard-coded rules.\nUses statistical techniques to improve with experience.\nExamples: spam filters, recommendation engines, credit scoring, fraud detection.\nüëâ ML is the how ‚Äî the toolbox for teaching machines.\nDL (Deep Learning) ‚Äì a subset of ML.\nThe breakthrough: neural networks with many layers that can automatically learn complex representations from raw data.\nRequires large datasets + high computational power (GPUs/TPUs).\nExamples: image recognition (CNNs), speech recognition (RNNs, Transformers), large language models (GPT, Gemini).\nüëâ DL is the engine ‚Äî the technology that powers today‚Äôs most advanced AI.\nüß† Visualization Think of it as nested circles:\nAI = broadest goal (machines that act smart) ML = subset (machines learn from data) DL = subset of ML (deep neural networks) graph TD AI[\"ü§ñ Artificial Intelligence (AI)Broadest goal ‚Äì machines that act smart\"] ML[\"üìä Machine Learning (ML)Subset ‚Äì machines learn from data\"] DL[\"üß† Deep Learning (DL)Subset of ML ‚Äì neural networks\"] AI --\u003e ML ML --\u003e DL üîç Why It Matters Not all AI is ML (e.g., a rule-based chess engine is AI but not ML).\nNot all ML is DL (e.g., logistic regression is ML but not DL).\nMost of today‚Äôs headline-grabbing AI breakthroughs (like ChatGPT or Stable Diffusion) are powered by Deep Learning.\nüëâ Understanding the distinction helps cut through hype and clarifies where different techniques fit in the AI landscape.\nüõ†Ô∏è AI in Software Engineering Practical uses for developers:\nCode completion \u0026 generation (Copilot, Tabnine) Test automation (unit tests, fuzzing) Bug detection (static analysis + AI) DevOps (incident prediction, scaling automation) üëâ AI is a developer productivity accelerator.\n‚öñÔ∏è Ethics, Bias \u0026 Responsible AI Bias in data ‚Üí unfair outputs. Hallucinations ‚Üí wrong but confident answers. Privacy risks ‚Üí sensitive data exposure. Accountability ‚Üí unclear ownership of AI decisions. üëâ Engineers must think beyond can we build this to should we build this.\nüí∞ Business \u0026 Market Applications AI drives billions in revenue across industries:\nHealthcare ‚Äì diagnostics, drug discovery Finance ‚Äì fraud detection, trading models Transportation ‚Äì autonomous driving, route optimization Media \u0026 entertainment ‚Äì content creation, personalization üöÄ How to Get Started with AI Learn Python (NumPy, Pandas). Explore ML libraries (scikit-learn, TensorFlow, PyTorch). Use cloud APIs (OpenAI, Anthropic, HuggingFace, Vertex AI). Build a toy project (chatbot, sentiment analysis, image classifier). üëâ Start small, learn by building.\nüéØ Future Trends Multimodal AI ‚Äì unified text, image, audio, video. AI Agents ‚Äì autonomous orchestration of tasks. Edge AI ‚Äì models running on devices, not just cloud. Domain-specific AI ‚Äì healthcare, law, finance. ü§ñ AI Agents: From Tools to Teammates Traditional AI models (like ChatGPT or Copilot) generate outputs when prompted.\nBut AI agents go further: they perceive, decide, and act in pursuit of goals.\nWhat Makes an AI Agent? Autonomy ‚Äì operates without step-by-step human instructions. Goal-oriented ‚Äì works toward objectives (e.g., ‚Äúbook me a trip to Berlin‚Äù). Adaptive ‚Äì learns from the environment or feedback loops. Interactive ‚Äì can collaborate with humans or other agents. Examples in Action Self-driving cars ‚Äì sense the road, plan routes, and control the vehicle. AI trading bots ‚Äì analyze markets and execute trades in real time. Customer support bots ‚Äì combine LLMs with APIs to resolve tickets. Multi-agent systems ‚Äì groups of agents cooperating in logistics or simulations. üí° Case Study: ClickHouse ran an experiment to see if large language models could act as on-call SREs, performing root cause analysis (RCA) during incidents. The results showed that while LLMs are helpful assistants in summarizing logs and suggesting hypotheses, they still fall short of replacing human SREs. This highlights a key theme: today‚Äôs AI agents augment human expertise rather than replace it in high-stakes domains.\nLLM-Powered Agents Modern frameworks (AutoGPT, LangChain agents, Microsoft Autogen) turn LLMs into agents with tools:\nSearch the web for live data. Write and execute code. Call APIs and databases. Plan multi-step workflows. Collaborate with other agents. üëâ This transforms AI from a chat assistant into a digital coworker capable of handling end-to-end tasks.\nWhy It Matters AI agents represent the next leap in AI evolution:\nAI ‚Äì the vision of intelligence in machines. ML/DL ‚Äì the methods that make learning possible. AI Agents ‚Äì the embodiment of intelligence in action. We‚Äôre entering an era where AI won‚Äôt just answer ‚Äî it will decide, act, and coordinate.\nThat shift will redefine software, business processes, and even how humans collaborate with machines.\nüîß MLOps: Making Machine Learning Production-Ready Building a machine learning model in a notebook is one thing. Running it safely, reliably, and at scale in the real world is another. That‚Äôs where MLOps (Machine Learning Operations) comes in.\nMLOps applies DevOps practices (automation, CI/CD, monitoring) to the machine learning lifecycle:\nData management ‚Äì version datasets, track quality.\nExperimentation ‚Äì manage models, hyperparameters, metrics.\nContinuous training (CT) ‚Äì retrain as data changes.\nDeployment ‚Äì push models into production APIs or batch pipelines.\nMonitoring ‚Äì detect drift, bias, and performance degradation.\nGovernance ‚Äì ensure compliance, reproducibility, and audit trails.\nTools in the ecosystem:\nPipelines: Kubeflow, Airflow, Metaflow\nExperiment tracking: MLflow, Weights \u0026 Biases\nDeployment: Docker, Kubernetes, Seldon\nMonitoring: EvidentlyAI, Prometheus, Grafana\nüëâ If ML is about building models, MLOps is about keeping them alive and useful in production.\nüì± Case Study: Mobile Teaching AI Assistant (Simplified) To connect theory with practice, let‚Äôs look at a simplified architecture for a Mobile Teaching AI Assistant ‚Äî a system designed to answer student questions, retrieve information, and provide context-aware explanations.\n{ data-lightbox=‚Äúai-post‚Äù }\nüîÑ Interaction Flow User Question ‚Äì A student asks a question via the mobile app.\nApp Backend ‚Äì The question is sent through a REST API to the AI backend.\nAssistant Engine ‚Äì The engine processes the request and decides whether to answer directly or call an external API.\nExternal AI Services ‚Äì Integration with providers like OpenAI, MS Azure, or translation APIs.\nResponse Delivery ‚Äì The final answer is sent back through the pipeline and displayed to the student in the mobile app.\nFeedback Loop ‚Äì Students can provide feedback (e.g., was the answer helpful?), improving the system over time.\nüèóÔ∏è Architecture Layer { data-lightbox=‚Äúai-post‚Äù }\nBehind the scenes, the assistant relies on a retrieval-augmented generation (RAG) pipeline:\nSources ‚Äì PDFs, lecture notes, articles, and other documents.\nChannels ‚Äì Ingestion pipelines that preprocess and clean the data.\nEmbeddings ‚Äì Text is transformed into vector embeddings using an embedding model.\nVector Store ‚Äì Stores embeddings for efficient semantic search.\nRetriever + LLM ‚Äì A student‚Äôs question is embedded, compared against the vector store, and the top-ranked results are passed into an LLM (like GPT).\nRanked Results ‚Äì The LLM generates an answer that combines retrieved knowledge with generative reasoning.\nüëâ This setup ensures answers are relevant, context-aware, and explainable rather than ‚Äúhallucinated.‚Äù\nüåü Why It Matters This Mobile AI Assistant illustrates how the concepts from earlier sections (AI, ML, DL, and MLOps) come together:\nAI provides the goal (a ‚Äúsmart‚Äù assistant).\nML/DL powers embeddings and LLM reasoning.\nMLOps ensures the system is reliable, monitored, and retrainable.\nDesign, Develop, Deploy lifecycle is visible: from model design ‚Üí backend development ‚Üí mobile deployment.\nüìå This kind of system shows how abstract AI concepts translate into tangible software solutions that can impact education, healthcare, finance, and beyond.\nüîÑ Wrapping Up AI = vision (smart systems) ML = method (learn from data) DL = breakthrough (neural nets at scale) Understanding these layers ‚Äî plus the risks, history, and market ‚Äî gives you the tools to cut through hype and apply AI effectively.\nüöÄ Follow me on norbix.dev for more insights on Go, Python, AI, system design, and engineering wisdom.\n","wordCount":"3907","inLanguage":"en","datePublished":"2025-09-04T18:00:00+02:00","dateModified":"2025-09-04T18:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://norbix.dev/posts/artificial-intelligence/"},"publisher":{"@type":"Organization","name":"norbix.dev - The log of my journey through code \u0026 software systems architecture","logo":{"@type":"ImageObject","url":"https://norbix.dev/favicon.ico"}}}</script></head><body><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://norbix.dev/ accesskey=h title="norbix.dev - The log of my journey through code & software systems architecture (Alt + H)">norbix.dev - The log of my journey through code & software systems architecture</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://norbix.dev/ title=Home><span>Home</span></a></li><li><a href=https://norbix.dev/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://norbix.dev/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://norbix.dev/about/ title=About><span>About</span></a></li><li><a href=https://norbix.dev/index.xml title="üì° RSS"><span>üì° RSS</span></a></li></ul></nav></header><main><main id=main><article class=post-single><header class=post-header><h1 class=post-title>Demystifying Artificial Intelligence: AI, Machine Learning, and Deep Learning</h1><span title='2025-09-04 18:00:00 +0200 +0200'>September 4, 2025</span></header><div class=post-content><nav class=table-of-contents><nav id=TableOfContents><ul><li><a href=#-a-brief-history-of-ai>üìú A Brief History of AI</a></li><li><a href=#-artificial-intelligence-the-big-picture>üß† Artificial Intelligence: The Big Picture</a><ul><li><a href=#-how-chatgpt-works-behind-the-scenes>ü§î How ChatGPT Works Behind the Scenes</a></li><li><a href=#-other-ai-models-competing-with-chatgpt>üîÑ Other AI Models Competing with ChatGPT</a></li></ul></li><li><a href=#-machine-learning-learning-from-data>üìä Machine Learning: Learning from Data</a></li><li><a href=#what-is-a-model-in-machine-learning>What is a Model in Machine Learning?</a><ul><li><a href=#parameters>Parameters</a></li><li><a href=#analogy>Analogy</a></li><li><a href=#how-it-works>How it works?</a></li><li><a href=#types-of-models>Types of models</a></li></ul></li><li><a href=#-common-ml-challenges-imbalanced-data--generalization>‚ö†Ô∏è Common ML Challenges: Imbalanced Data & Generalization</a><ul><li><a href=#1-imbalanced-classes>1. Imbalanced Classes</a></li><li><a href=#2-overfitting>2. Overfitting</a></li><li><a href=#3-underfitting>3. Underfitting</a></li><li><a href=#example>Example</a></li><li><a href=#-connecting-the-dots>üß† Connecting the Dots</a></li></ul></li><li><a href=#-deep-learning-the-neural-revolution>ü§ñ Deep Learning: The Neural Revolution</a><ul><li><a href=#-deep-learning-the-neural-revolution-1>ü§ñ Deep Learning: The Neural Revolution</a></li></ul></li><li><a href=#-key-ai-techniques-beyond-ml>üõ†Ô∏è Key AI Techniques Beyond ML</a></li><li><a href=#-ai-vs-ml-vs-dl-a-mental-model>‚öñÔ∏è AI vs. ML vs. DL: A Mental Model</a><ul><li><a href=#-visualization>üß† Visualization</a></li><li><a href=#-why-it-matters>üîç Why It Matters</a></li></ul></li><li><a href=#-ai-in-software-engineering>üõ†Ô∏è AI in Software Engineering</a></li><li><a href=#-ethics-bias--responsible-ai>‚öñÔ∏è Ethics, Bias & Responsible AI</a></li><li><a href=#-business--market-applications>üí∞ Business & Market Applications</a></li><li><a href=#-how-to-get-started-with-ai>üöÄ How to Get Started with AI</a></li><li><a href=#-future-trends>üéØ Future Trends</a></li><li><a href=#-ai-agents-from-tools-to-teammates>ü§ñ AI Agents: From Tools to Teammates</a><ul><li><a href=#what-makes-an-ai-agent>What Makes an AI Agent?</a></li><li><a href=#examples-in-action>Examples in Action</a></li><li><a href=#llm-powered-agents>LLM-Powered Agents</a></li><li><a href=#why-it-matters>Why It Matters</a></li></ul></li><li><a href=#-mlops-making-machine-learning-production-ready>üîß MLOps: Making Machine Learning Production-Ready</a></li><li><a href=#-case-study-mobile-teaching-ai-assistant-simplified>üì± Case Study: Mobile Teaching AI Assistant (Simplified)</a><ul><li><a href=#-interaction-flow>üîÑ Interaction Flow</a></li><li><a href=#-architecture-layer>üèóÔ∏è Architecture Layer</a></li><li><a href=#-why-it-matters-1>üåü Why It Matters</a></li></ul></li><li><a href=#-wrapping-up>üîÑ Wrapping Up</a></li></ul></nav></nav><p><img loading=lazy src=banner.jpg alt=banner></p><p><strong>&ldquo;Artificial Intelligence isn‚Äôt about replacing humans. It‚Äôs about amplifying human potential.&rdquo;</strong></p><p>Artificial Intelligence (AI) is one of the most transformative forces in technology today. From recommendation engines on Netflix to self-driving cars and generative models like ChatGPT, AI is shaping how we work, live, and create.</p><p>But <strong>AI is often misunderstood</strong>. Is it the same as machine learning? Where does deep learning fit? Let‚Äôs break it down.</p><hr><h2 id=-a-brief-history-of-ai>üìú A Brief History of AI</h2><ul><li><strong>1950s</strong> ‚Äì Alan Turing proposes the Turing Test. Early symbolic <code>AI</code> emerges.</li><li><strong>1980s‚Äì1990s</strong> ‚Äì Expert systems and rule-based knowledge engines dominate.</li><li><strong>2000s</strong> ‚Äì Rise of statistical machine learning thanks to bigger datasets.</li><li><strong>2010s</strong> ‚Äì Deep learning revolution with <code>neural networks</code> and <code>GPUs</code>.</li><li><strong>2020s</strong> ‚Äì Generative <code>AI</code> (<code>ChatGPT</code>, <code>Claude</code>, <code>Gemini</code>) makes <code>AI</code> mainstream.</li></ul><p>üîπ <strong>Tip:</strong> AI has decades of research behind it ‚Äî what feels ‚Äúnew‚Äù is the scale and accessibility today.</p><hr><h2 id=-artificial-intelligence-the-big-picture>üß† Artificial Intelligence: The Big Picture</h2><p><strong>Artificial Intelligence (AI)</strong> is the broad field focused on creating systems that mimic human intelligence.</p><p>Examples include:</p><ul><li>Rule-based systems (e.g., chess engines from the 1980s)</li><li>Natural language processing (chatbots, translators)</li><li>Computer vision (face recognition, object detection)</li><li>Robotics and autonomous systems</li></ul><p>AI doesn‚Äôt always require learning. A simple rule-based expert system is AI, even if it doesn‚Äôt adapt over time.</p><p>üîπ <strong>Tip:</strong> Think of AI as the <em>goal</em> ‚Äî making machines ‚Äúsmart.‚Äù</p><hr><h3 id=-how-chatgpt-works-behind-the-scenes>ü§î How ChatGPT Works Behind the Scenes</h3><p>One of today‚Äôs most visible applications of AI is <strong>ChatGPT</strong>, a large language model built using deep learning. Here‚Äôs how it works at a high level:</p><ol><li><strong>Training on huge datasets</strong> ‚Äì Learns statistical patterns from books, code, and the web.</li><li><strong>Neural network architecture</strong> ‚Äì Uses <em>Transformers</em> to capture relationships between words.</li><li><strong>Token prediction</strong> ‚Äì Predicts the most likely next word (token) in a sequence.</li><li><strong>Fine-tuning & RLHF</strong> ‚Äì Reinforcement learning from human feedback aligns responses.</li><li><strong>Inference</strong> ‚Äì At runtime, your input is converted into tokens, processed through billions of neural weights, and output as natural language.</li></ol><p>üîπ <strong>Tip:</strong> ChatGPT doesn‚Äôt ‚Äúunderstand‚Äù like a human. It‚Äôs a probabilistic pattern-matching engine.</p><hr><h3 id=-other-ai-models-competing-with-chatgpt>üîÑ Other AI Models Competing with ChatGPT</h3><p>The market is full of competitors, each with different strengths:</p><ul><li><strong>Claude (Anthropic):</strong> Long context, reasoning, ethical design.</li><li><strong>Google Gemini:</strong> Multimodal (text, image, audio, video).</li><li><strong>xAI Grok:</strong> Multimodal with real-time search, integrated in X/Tesla.</li><li><strong>Perplexity:</strong> AI + live web search with citations.</li><li><strong>Microsoft Copilot:</strong> Embedded in Office/Teams with GPT-4 Turbo.</li><li><strong>Meta AI (LLaMA):</strong> Social/media apps, open research focus.</li><li><strong>DeepSeek (China):</strong> Efficiency-driven, strong benchmarks.</li><li><strong>Mistral AI (EU):</strong> Open-source, long context, developer-friendly.</li><li><strong>Moonshot AI (China):</strong> Large trillion-parameter ‚ÄúKimi‚Äù models.</li><li><strong>YandexGPT:</strong> Russian-focused business integrations.</li></ul><table><thead><tr><th>Model</th><th>Strengths</th><th>Best For</th></tr></thead><tbody><tr><td>Claude</td><td>Long context, reasoning</td><td>Research & enterprise workflows</td></tr><tr><td>Gemini</td><td>Multimodal, Google ecosystem</td><td>Cross-media AI</td></tr><tr><td>Grok</td><td>Real-time retrieval, reasoning</td><td>Social/voice-first apps</td></tr><tr><td>Perplexity</td><td>Citations, fact-checking</td><td>Research and knowledge tasks</td></tr><tr><td>Copilot</td><td>Deep MS integration</td><td>Productivity workflows</td></tr><tr><td>Meta AI</td><td>Social media ecosystem</td><td>Chat & consumer interaction</td></tr><tr><td>DeepSeek</td><td>Energy-efficient reasoning</td><td>Scale-sensitive applications</td></tr><tr><td>Mistral</td><td>Open-source, flexible</td><td>Developer tooling & customization</td></tr><tr><td>Moonshot AI</td><td>Massive models, multimodal</td><td>Cutting-edge innovation</td></tr><tr><td>YandexGPT</td><td>Localized enterprise AI</td><td>Russian-language businesses</td></tr></tbody></table><p>üîπ <strong>Tip:</strong> Pick your AI model based on <strong>ecosystem fit</strong> (Google, Microsoft, Meta), <strong>task type</strong> (research vs creative), and <strong>control</strong> (open vs closed source).</p><hr><h2 id=-machine-learning-learning-from-data>üìä Machine Learning: Learning from Data</h2><p><strong>Machine Learning (ML)</strong> is a subset of AI. Instead of hard-coding rules, ML algorithms learn from data and improve with exposure.</p><p>Applications: spam filters, predictive maintenance, fraud detection, recommendations.</p><p>Methods: regression, decision trees, clustering, reinforcement learning.</p><p>üîπ <strong>Tip:</strong> ML is the <em>toolbox</em> that powers modern AI.</p><hr><h2 id=what-is-a-model-in-machine-learning>What is a Model in Machine Learning?</h2><p>A model in machine learning is basically a mathematical function that:</p><ul><li><p>Takes inputs (features, e.g. hours_studied).</p></li><li><p>Produces an output (prediction, e.g. expected score).</p></li><li><p>Learns the relationship between inputs and outputs by looking at examples in data.</p></li><li><p>The logic or formula type the machine learning system is trying to learn.</p></li></ul><h3 id=parameters>Parameters</h3><ul><li><p>The specific values in that logic that the training process discovers.</p></li><li><p>Imagine you want to guess a student‚Äôs exam score based on how many hours they studied.</p><ul><li><p>You collect data:</p><p>create table</p><table><thead><tr><th>hours_studied</th><th>score</th></tr></thead><tbody><tr><td>1</td><td>50</td></tr><tr><td>2</td><td>60</td></tr><tr><td>3</td><td>70</td></tr></tbody></table><p>A model could be as simple as the rule:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-matlab data-lang=matlab><span class=line><span class=cl><span class=n>score</span> <span class=p>=</span> <span class=mi>10</span> <span class=o>*</span> <span class=n>hours_studied</span> <span class=o>+</span> <span class=mi>40</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Model = &ldquo;linear relationship between hours and score&rdquo;</li><li>Parameters = slope = 10 and intercept = 40</li></ul></li></ul></li></ul><h3 id=analogy>Analogy</h3><ul><li><p>Model = the shape of the recipe (e.g., ‚Äúbake a cake‚Äù).</p><ul><li>That‚Äôs just a formula ‚Äî and that‚Äôs what a model is.</li></ul></li><li><p>Parameters = the exact ingredient amounts (200g flour, 2 eggs, 100g sugar).</p></li><li><p>Training = adjusting ingredient amounts until the cake tastes right.</p></li><li><p>A model is the logic found.</p></li><li><p>Parameters are the variables (numbers) that make that logic concrete</p></li></ul><h3 id=how-it-works>How it works?</h3><ul><li><p>You don‚Äôt hand-code the rules (like if hours > 5 then good score).</p></li><li><p>Instead, you give the model examples (inputs + correct outputs).</p></li><li><p>The training process adjusts internal parameters until the model finds rules that best fit the data.</p></li></ul><p>So yes ‚Äî in a way, the model is ‚Äúcreating its own logic‚Äù.</p><p>You don&rsquo;t write:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>hours</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>10</span> <span class=o>*</span> <span class=n>hours</span> <span class=o>+</span> <span class=mi>40</span>
</span></span></code></pre></td></tr></table></div></div><p>The model discovers that rule by itself, because that line best fits the data.</p><h3 id=types-of-models>Types of models</h3><ol><li>Linear Regression ‚Üí learns straight-line formulas.</li></ol><ul><li><p>Linear Regression ‚Üí predicts continuous values with straight-line formulas.</p></li><li><p>Logistic Regression ‚Üí predicts probabilities/classes with a linear boundary.</p></li><li><p>Linear SVM ‚Üí finds a straight hyperplane to separate classes.</p></li><li><p>üëâ Good for: simple, linearly separable problems.</p></li><li><p>üëâ Limitation: can‚Äôt capture curves, waves, or complex shapes ‚Üí risk of underfitting.</p></li></ul><ol><li>Decision Trees ‚Üí learns rules like ‚Äúif amount > 1000 then fraud.‚Äù</li></ol><ul><li><p>Decision Trees ‚Üí split data with simple rules (if amount > 1000 ‚Üí fraud).</p></li><li><p>Random Forests ‚Üí many trees combined ‚Üí better accuracy, less overfitting.</p></li><li><p>Gradient Boosted Trees (XGBoost, LightGBM, CatBoost) ‚Üí trees built in sequence to fix each other‚Äôs mistakes.</p></li><li><p>üëâ Good for: tabular data (transactions, customer info).</p></li><li><p>üëâ Strength: handles non-linear patterns, interactions.</p></li></ul><ol><li>Neural Networks ‚Üí complex layered functions that can learn images, text, etc.</li></ol><ul><li><p>Simple feed-forward networks (MLPs) ‚Üí capture non-linear patterns.</p></li><li><p>CNNs (Convolutional Neural Networks) ‚Üí great for images.</p></li><li><p>RNNs / LSTMs / Transformers ‚Üí great for sequences, text, time series.</p></li><li><p>üëâ Good for: complex patterns (vision, NLP, speech).</p></li><li><p>üëâ Strength: very flexible, can approximate almost any function.</p></li><li><p>üëâ Limitation: need lots of data + compute.</p></li></ul><p>üëâ In short: A model is a trained function that tries to map inputs ‚Üí outputs based on patterns it finds in the data.</p><ol><li>Probabilistic Models</li></ol><ul><li><p>Na√Øve Bayes ‚Üí simple, based on probability rules.</p></li><li><p>Hidden Markov Models ‚Üí sequential, time-series modeling.</p></li><li><p>üëâ Good for: small datasets, text classification, spam detection.</p></li></ul><ol><li>Clustering / Unsupervised Models</li></ol><ul><li><p>K-Means ‚Üí groups similar points.</p></li><li><p>Hierarchical Clustering ‚Üí builds a tree of clusters.</p></li><li><p>DBSCAN ‚Üí finds clusters of arbitrary shapes.</p></li><li><p>üëâ Good for: when you don‚Äôt have labels (unsupervised learning).</p></li></ul><h4 id=-how-to-choose>‚ö° How to choose?</h4><ul><li><p>If the pattern is simple & linear ‚Üí linear regression / logistic regression.</p></li><li><p>If the data has rules & thresholds ‚Üí tree-based models.</p></li><li><p>If the problem is very complex (images, text, audio, high-dimensional data) ‚Üí neural networks.</p></li><li><p>If you have no labels ‚Üí clustering methods.</p></li></ul><hr><h2 id=-common-ml-challenges-imbalanced-data--generalization>‚ö†Ô∏è Common ML Challenges: Imbalanced Data & Generalization</h2><h3 id=1-imbalanced-classes>1. Imbalanced Classes</h3><h4 id=-problem>üö® Problem</h4><ul><li><p>Happens when one class dominates the dataset.</p></li><li><p>Example: Fraud detection ‚Üí 99% ‚Äúlegit‚Äù vs 1% ‚Äúfraud‚Äù.</p></li><li><p>If you train a classifier, it might always predict the majority class and still get 99% accuracy.</p></li></ul><h4 id=example-dataset>Example Dataset</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;dataset&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx001&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>120.50</span><span class=p>,</span> <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;NY&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx002&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>80.00</span><span class=p>,</span>  <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx003&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>75.00</span><span class=p>,</span>  <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;TX&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx004&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>200.00</span><span class=p>,</span> <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;NY&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx005&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>950.00</span><span class=p>,</span> <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;FL&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx006&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>20.00</span><span class=p>,</span>  <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;WA&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx007&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>500.00</span><span class=p>,</span> <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;IL&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx008&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>50.00</span><span class=p>,</span>  <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;NV&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx009&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>100.00</span><span class=p>,</span> <span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;NY&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;legit&#34;</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;transaction_id&#34;</span><span class=p>:</span> <span class=s2>&#34;tx010&#34;</span><span class=p>,</span> <span class=nt>&#34;amount&#34;</span><span class=p>:</span> <span class=mf>5000.00</span><span class=p>,</span><span class=nt>&#34;location&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span> <span class=nt>&#34;label&#34;</span><span class=p>:</span> <span class=s2>&#34;fraud&#34;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=-solutions>üõ†Ô∏è Solutions</h4><ol><li><p>Resampling the dataset</p><ul><li><p>Oversampling minority class (e.g., SMOTE ‚Äì Synthetic Minority Oversampling Technique).</p></li><li><p>Undersampling majority class to balance the distribution.</p></li></ul></li><li><p>Adjusting class weights</p><ul><li>Penalize mistakes on the minority class more heavily (supported in many ML frameworks).</li></ul></li><li><p>Choosing the right metrics</p><ul><li><p>Accuracy is misleading. Better: Precision, Recall, F1-score, ROC-AUC, PR-AUC.</p></li><li><p>For fraud, often maximize recall (catch as many frauds as possible) at the expense of some false positives.</p></li></ul></li></ol><p>üëâ Key interview takeaway: ‚ÄúWith imbalanced data, I focus on resampling, adjusting class weights, and using metrics beyond accuracy, like precision, recall, and ROC-AUC.‚Äù</p><h3 id=2-overfitting>2. Overfitting</h3><h4 id=-problem-1>üö® Problem</h4><ul><li><p>Model learns too much from training data (including noise and quirks).</p></li><li><p>Great on training set, bad on unseen/test data.</p></li></ul><h4 id=-symptoms>üõ†Ô∏è Symptoms</h4><ul><li><p>High training accuracy, low validation/test accuracy.</p></li><li><p>Loss continues dropping on training, but rises on validation (classic overfitting curve).</p></li></ul><h4 id=example-dataset-1>Example Dataset</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;training_set&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s001&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>50</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s002&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>60</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s003&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>70</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s004&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>65</span> <span class=p>},</span> 
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s005&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>80</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;test_set&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s101&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>85</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s102&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>7</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>90</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;student_id&#34;</span><span class=p>:</span> <span class=s2>&#34;s103&#34;</span><span class=p>,</span> <span class=nt>&#34;hours_studied&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span> <span class=nt>&#34;score&#34;</span><span class=p>:</span> <span class=mi>95</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>What‚Äôs happening here?</p><ul><li><p>In the training data, look at &ldquo;hours_studied&rdquo;: 4.</p><ul><li><p>Instead of following the trend (50 ‚Üí 60 ‚Üí 70 ‚Üí 80‚Ä¶), the score drops to 65.</p></li><li><p>This is noise.</p></li></ul></li><li><p>A complex model might ‚Äúmemorize‚Äù that drop and think:</p><ul><li>‚ÄúStudying 4 hours actually lowers your score.‚Äù</li></ul></li><li><p>On the test set, where the true trend continues upward (6 ‚Üí 85, 7 ‚Üí 90, 8 ‚Üí 95), the model makes wrong predictions because it learned the noise.</p></li></ul><p>That‚Äôs overfitting:</p><ul><li><p>‚úÖ Training accuracy = high (because it memorized everything).</p></li><li><p>‚ùå Test accuracy = low (because it didn‚Äôt generalize the real rule).</p></li></ul><h4 id=-solutions-1>üõ†Ô∏è Solutions</h4><ul><li><p>Regularization: L1 (sparsity), L2 (weight decay).</p></li><li><p>Dropout (turning off random neurons during training).</p></li><li><p>Early stopping (halt training when validation loss worsens).</p></li><li><p>Simpler model (reduce number of parameters).</p></li><li><p>More data / data augmentation (especially in image tasks).</p></li></ul><p>üëâ Key interview takeaway: ‚ÄúOverfitting is when the model memorizes instead of generalizing. I fight it with regularization, dropout, early stopping, and more data.‚Äù</p><h3 id=3-underfitting>3. Underfitting</h3><h4 id=-problem-2>üö® Problem</h4><ul><li><p>Model is too simple to capture the underlying patterns.</p></li><li><p>Poor performance on both training and test sets.</p></li></ul><h4 id=-symptoms-1>üõ†Ô∏è Symptoms</h4><ul><li><p>Both training and validation accuracy are low.</p></li><li><p>Loss is high and doesn‚Äôt improve.</p></li></ul><h3 id=example>Example</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;dataset&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>1</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>4</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>9</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>16</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>25</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>36</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>7</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>49</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>64</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>9</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>81</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span> <span class=nt>&#34;x&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span> <span class=nt>&#34;y&#34;</span><span class=p>:</span> <span class=mi>100</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><p>Underfitting = the model is too simple, so it fails to learn even the obvious pattern.</p></li><li><p>Example: the real relationship is non-linear (curved), but the model tries to force a straight line.</p></li></ul><h4 id=why-this-shows-underfitting>Why this shows underfitting</h4><p>The true pattern is quadratic:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-matlab data-lang=matlab><span class=line><span class=cl><span class=n>y</span><span class=p>=</span><span class=n>x2</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><p>If you force a linear model (straight line, e.g. y = ax + b), it won‚Äôt fit well:</p><ul><li><p>At small x, it predicts too high.</p></li><li><p>At large x, it predicts too low.</p></li></ul></li><li><p>Result:</p><ul><li><p>‚ùå Training accuracy = low.</p></li><li><p>‚ùå Test accuracy = low.</p></li></ul></li></ul><p>That‚Äôs underfitting ‚Äî the model is too simple for the data‚Äôs complexity.</p><h4 id=-rule-of-thumb>‚ö° Rule of thumb</h4><ul><li><p>If you know the pattern is quadratic ‚Üí Polynomial Regression is the cleanest choice.</p></li><li><p>If you suspect it could be more complex ‚Üí go with trees or a small neural net.</p></li></ul><h4 id=-solutions-2>üõ†Ô∏è Solutions</h4><ul><li><p>Use a more complex model (more layers, deeper tree, etc.).</p></li><li><p>Train longer (more epochs, better learning rate).</p></li><li><p>Feature engineering (add informative features).</p></li><li><p>Reduce regularization (too strong regularization may cause underfitting).</p></li></ul><p>üëâ Key interview takeaway:‚ÄúUnderfitting is when the model is too simple. To fix it, I increase model complexity, add better features, or train longer.‚Äù</p><h3 id=-connecting-the-dots>üß† Connecting the Dots</h3><ul><li><p>Imbalanced classes: The data distribution is skewed ‚Üí accuracy is misleading.</p></li><li><p>Overfitting: The model is too complex ‚Üí memorizes instead of generalizing.</p></li><li><p>Underfitting: The model is too simple ‚Üí fails to learn meaningful patterns.</p></li></ul><hr><h2 id=-deep-learning-the-neural-revolution>ü§ñ Deep Learning: The Neural Revolution</h2><h3 id=-deep-learning-the-neural-revolution-1>ü§ñ Deep Learning: The Neural Revolution</h3><p>Deep Learning (DL) is a subset of ML that relies on artificial neural networks (ANNs) with many layers. These layers allow the model to learn increasingly complex representations of data ‚Äî from edges in an image to entire concepts like ‚Äúcat‚Äù or ‚Äúcar.‚Äù</p><h4 id=-what-are-neural-networks>üß© What Are Neural Networks?</h4><ul><li><p>Inspired by biology ‚Äì loosely modeled after neurons in the human brain.</p></li><li><p>Structure ‚Äì input layer (data), hidden layers (transformations), output layer (prediction).</p></li><li><p>Connections ‚Äì each neuron has weights and biases, adjusted during training.</p></li><li><p>Activation functions ‚Äì nonlinear transformations (ReLU, sigmoid, tanh, softmax) that let networks learn complex relationships.</p></li></ul><p>üëâ Without activation functions, a neural network would just be a fancy linear regression.</p><h4 id=-how-neural-networks-learn>üîÑ How Neural Networks Learn</h4><p>The training process follows a loop:</p><ol><li><p>Forward pass ‚Äì input flows through layers, producing an output.</p></li><li><p>Loss function ‚Äì measures how far the prediction is from the correct answer.</p></li><li><p>Backward pass (backpropagation) ‚Äì calculates gradients of the loss with respect to weights.</p></li><li><p>Optimization (gradient descent) ‚Äì updates weights to reduce error.</p></li></ol><p>This cycle repeats thousands or millions of times until the network converges on good parameters.</p><h4 id=-types-of-neural-networks>üèóÔ∏è Types of Neural Networks</h4><ul><li><p>Feedforward Networks (MLP) ‚Äì simplest form, fully connected layers.</p></li><li><p>Convolutional Neural Networks (CNNs) ‚Äì specialized for images and spatial data (e.g., object detection, face recognition).</p></li><li><p>Recurrent Neural Networks (RNNs) ‚Äì designed for sequences (e.g., speech, text, time-series).</p></li><li><p>Transformers ‚Äì modern architecture for language, vision, and multimodal tasks (powering GPT, Gemini, Claude).</p></li></ul><h4 id=-why-deep-learning-works-so-well>‚ö° Why Deep Learning Works So Well</h4><ul><li><p>Learns hierarchical features automatically (no manual feature engineering).</p></li><li><p>Scales with big data and powerful hardware (GPUs/TPUs).</p></li><li><p>Excels at unstructured data: images, audio, text.</p></li></ul><h4 id=-real-world-applications>üåç Real-World Applications</h4><ul><li><p>Image recognition ‚Äì self-driving cars, medical imaging.</p></li><li><p>Speech recognition ‚Äì voice assistants, transcription.</p></li><li><p>Natural language processing ‚Äì chatbots, translation, sentiment analysis.</p></li><li><p>Generative AI ‚Äì LLMs (ChatGPT, Claude), diffusion models (Stable Diffusion, MidJourney).</p></li></ul><p>üîπ Tip: Deep learning is what made AI feel magical ‚Äî moving from ‚Äúmachines that calculate‚Äù to ‚Äúmachines that see, listen, and talk.‚Äù</p><h4 id=simple-diagram-of-a-feedforward-neural-network-with-one-hidden-layer>Simple diagram of a feedforward neural network with one hidden layer</h4><ul><li><strong>Input layer</strong>: features (e.g., pixels, words, measurements).</li><li><strong>Hidden layer</strong>: neurons transform inputs using weights + activation functions.</li><li><strong>Output layer</strong>: final prediction (classification, regression, etc.).</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph LR
    subgraph Input[&#34;Input Layer&#34;]
        I1[&#34;x‚ÇÅ&#34;]
        I2[&#34;x‚ÇÇ&#34;]
        I3[&#34;x‚ÇÉ&#34;]
    end

    subgraph Hidden[&#34;Hidden Layer (Neurons)&#34;]
        H1[&#34;h‚ÇÅ&#34;]
        H2[&#34;h‚ÇÇ&#34;]
        H3[&#34;h‚ÇÉ&#34;]
    end

    subgraph Output[&#34;Output Layer&#34;]
        O1[&#34;≈∑ (prediction)&#34;]
    end

    I1 --&gt; H1
    I1 --&gt; H2
    I1 --&gt; H3
    I2 --&gt; H1
    I2 --&gt; H2
    I2 --&gt; H3
    I3 --&gt; H1
    I3 --&gt; H2
    I3 --&gt; H3

    H1 --&gt; O1
    H2 --&gt; O1
    H3 --&gt; O1
</code></pre><hr><h2 id=-key-ai-techniques-beyond-ml>üõ†Ô∏è Key AI Techniques Beyond ML</h2><p>AI also includes:</p><ul><li><strong>Search algorithms</strong> (A*, minimax in games)</li><li><strong>Planning systems</strong> (robotics, logistics scheduling)</li><li><strong>Knowledge graphs & reasoning</strong> (semantic web, ontologies)</li><li><strong>Rule-based expert systems</strong> (if-else driven logic engines)</li></ul><p>üëâ Not all AI is ML ‚Äî classic approaches still power many systems.</p><hr><h2 id=-ai-vs-ml-vs-dl-a-mental-model>‚öñÔ∏è AI vs. ML vs. DL: A Mental Model</h2><p>One of the biggest sources of confusion in tech discussions is the relationship between Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL). The simplest way to think about it is as nested circles:</p><ul><li><p>AI (Artificial Intelligence) ‚Äì the broadest concept.</p><ul><li><p>The goal: make machines simulate human intelligence.</p></li><li><p>Includes both learning systems and rule-based systems.</p></li><li><p>Examples: expert systems, knowledge graphs, search algorithms, game-playing bots, natural language processing, robotics.</p></li><li><p>üëâ AI is the what ‚Äî the ambition of making machines act smart.</p></li></ul></li><li><p>ML (Machine Learning) ‚Äì a subset of AI.</p><ul><li><p>The method: algorithms that learn patterns from data instead of relying on hard-coded rules.</p></li><li><p>Uses statistical techniques to improve with experience.</p></li><li><p>Examples: spam filters, recommendation engines, credit scoring, fraud detection.</p></li><li><p>üëâ ML is the how ‚Äî the toolbox for teaching machines.</p></li></ul></li><li><p>DL (Deep Learning) ‚Äì a subset of ML.</p><ul><li><p>The breakthrough: neural networks with many layers that can automatically learn complex representations from raw data.</p></li><li><p>Requires large datasets + high computational power (GPUs/TPUs).</p></li><li><p>Examples: image recognition (CNNs), speech recognition (RNNs, Transformers), large language models (GPT, Gemini).</p></li><li><p>üëâ DL is the engine ‚Äî the technology that powers today‚Äôs most advanced AI.</p></li></ul></li></ul><h3 id=-visualization>üß† Visualization</h3><p>Think of it as nested circles:</p><ul><li><strong>AI</strong> = broadest goal (machines that act smart)</li><li><strong>ML</strong> = subset (machines learn from data)</li><li><strong>DL</strong> = subset of ML (deep neural networks)</li></ul><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    AI[&#34;ü§ñ Artificial Intelligence (AI)&lt;br/&gt;Broadest goal ‚Äì machines that act smart&#34;]
    ML[&#34;üìä Machine Learning (ML)&lt;br/&gt;Subset ‚Äì machines learn from data&#34;]
    DL[&#34;üß† Deep Learning (DL)&lt;br/&gt;Subset of ML ‚Äì neural networks&#34;]
    
    AI --&gt; ML
    ML --&gt; DL
</code></pre><h3 id=-why-it-matters>üîç Why It Matters</h3><ul><li><p>Not all AI is ML (e.g., a rule-based chess engine is AI but not ML).</p></li><li><p>Not all ML is DL (e.g., logistic regression is ML but not DL).</p></li><li><p>Most of today‚Äôs headline-grabbing AI breakthroughs (like ChatGPT or Stable Diffusion) are powered by Deep Learning.</p></li></ul><p>üëâ Understanding the distinction helps cut through hype and clarifies where different techniques fit in the AI landscape.</p><hr><h2 id=-ai-in-software-engineering>üõ†Ô∏è AI in Software Engineering</h2><p>Practical uses for developers:</p><ul><li><strong>Code completion & generation</strong> (Copilot, Tabnine)</li><li><strong>Test automation</strong> (unit tests, fuzzing)</li><li><strong>Bug detection</strong> (static analysis + AI)</li><li><strong>DevOps</strong> (incident prediction, scaling automation)</li></ul><p>üëâ AI is a <strong>developer productivity accelerator</strong>.</p><hr><h2 id=-ethics-bias--responsible-ai>‚öñÔ∏è Ethics, Bias & Responsible AI</h2><ul><li><strong>Bias in data</strong> ‚Üí unfair outputs.</li><li><strong>Hallucinations</strong> ‚Üí wrong but confident answers.</li><li><strong>Privacy risks</strong> ‚Üí sensitive data exposure.</li><li><strong>Accountability</strong> ‚Üí unclear ownership of AI decisions.</li></ul><p>üëâ Engineers must think beyond <em>can we build this</em> to <em>should we build this</em>.</p><hr><h2 id=-business--market-applications>üí∞ Business & Market Applications</h2><p>AI drives billions in revenue across industries:</p><ul><li><strong>Healthcare</strong> ‚Äì diagnostics, drug discovery</li><li><strong>Finance</strong> ‚Äì fraud detection, trading models</li><li><strong>Transportation</strong> ‚Äì autonomous driving, route optimization</li><li><strong>Media & entertainment</strong> ‚Äì content creation, personalization</li></ul><hr><h2 id=-how-to-get-started-with-ai>üöÄ How to Get Started with AI</h2><ol><li>Learn Python (NumPy, Pandas).</li><li>Explore ML libraries (scikit-learn, TensorFlow, PyTorch).</li><li>Use cloud APIs (OpenAI, Anthropic, HuggingFace, Vertex AI).</li><li>Build a toy project (chatbot, sentiment analysis, image classifier).</li></ol><p>üëâ Start small, learn by building.</p><hr><h2 id=-future-trends>üéØ Future Trends</h2><ul><li><strong>Multimodal AI</strong> ‚Äì unified text, image, audio, video.</li><li><strong>AI Agents</strong> ‚Äì autonomous orchestration of tasks.</li><li><strong>Edge AI</strong> ‚Äì models running on devices, not just cloud.</li><li><strong>Domain-specific AI</strong> ‚Äì healthcare, law, finance.</li></ul><hr><h2 id=-ai-agents-from-tools-to-teammates>ü§ñ AI Agents: From Tools to Teammates</h2><p>Traditional AI models (like ChatGPT or Copilot) generate outputs when prompted.<br>But <strong>AI agents</strong> go further: they <em>perceive, decide, and act</em> in pursuit of goals.</p><h3 id=what-makes-an-ai-agent>What Makes an AI Agent?</h3><ul><li><strong>Autonomy</strong> ‚Äì operates without step-by-step human instructions.</li><li><strong>Goal-oriented</strong> ‚Äì works toward objectives (e.g., ‚Äúbook me a trip to Berlin‚Äù).</li><li><strong>Adaptive</strong> ‚Äì learns from the environment or feedback loops.</li><li><strong>Interactive</strong> ‚Äì can collaborate with humans or other agents.</li></ul><h3 id=examples-in-action>Examples in Action</h3><ul><li><strong>Self-driving cars</strong> ‚Äì sense the road, plan routes, and control the vehicle.</li><li><strong>AI trading bots</strong> ‚Äì analyze markets and execute trades in real time.</li><li><strong>Customer support bots</strong> ‚Äì combine LLMs with APIs to resolve tickets.</li><li><strong>Multi-agent systems</strong> ‚Äì groups of agents cooperating in logistics or simulations.</li></ul><p>üí° <strong>Case Study:</strong> <a href=https://clickhouse.com/blog/llm-observability-challenge>ClickHouse ran an experiment</a> to see if large language models could act as on-call SREs, performing root cause analysis (RCA) during incidents. The results showed that while LLMs are <em>helpful assistants</em> in summarizing logs and suggesting hypotheses, they still fall short of replacing human SREs. This highlights a key theme: today‚Äôs AI agents <strong>augment human expertise rather than replace it</strong> in high-stakes domains.</p><h3 id=llm-powered-agents>LLM-Powered Agents</h3><p>Modern frameworks (AutoGPT, LangChain agents, Microsoft Autogen) turn LLMs into <strong>agents with tools</strong>:</p><ul><li>Search the web for live data.</li><li>Write and execute code.</li><li>Call APIs and databases.</li><li>Plan multi-step workflows.</li><li>Collaborate with other agents.</li></ul><p>üëâ This transforms AI from a <strong>chat assistant</strong> into a <strong>digital coworker</strong> capable of handling end-to-end tasks.</p><h3 id=why-it-matters>Why It Matters</h3><p>AI agents represent the next leap in AI evolution:</p><ul><li><strong>AI</strong> ‚Äì the vision of intelligence in machines.</li><li><strong>ML/DL</strong> ‚Äì the methods that make learning possible.</li><li><strong>AI Agents</strong> ‚Äì the embodiment of intelligence in <em>action</em>.</li></ul><p>We‚Äôre entering an era where AI won‚Äôt just answer ‚Äî it will <strong>decide, act, and coordinate</strong>.<br>That shift will redefine software, business processes, and even how humans collaborate with machines.</p><hr><h2 id=-mlops-making-machine-learning-production-ready>üîß MLOps: Making Machine Learning Production-Ready</h2><p>Building a machine learning model in a notebook is one thing. Running it safely, reliably, and at scale in the real world is another. That‚Äôs where MLOps (Machine Learning Operations) comes in.</p><p>MLOps applies DevOps practices (automation, CI/CD, monitoring) to the machine learning lifecycle:</p><ol><li><p>Data management ‚Äì version datasets, track quality.</p></li><li><p>Experimentation ‚Äì manage models, hyperparameters, metrics.</p></li><li><p>Continuous training (CT) ‚Äì retrain as data changes.</p></li><li><p>Deployment ‚Äì push models into production APIs or batch pipelines.</p></li><li><p>Monitoring ‚Äì detect drift, bias, and performance degradation.</p></li><li><p>Governance ‚Äì ensure compliance, reproducibility, and audit trails.</p></li></ol><p>Tools in the ecosystem:</p><ul><li><p>Pipelines: <code>Kubeflow</code>, <code>Airflow</code>, <code>Metaflow</code></p></li><li><p>Experiment tracking: <code>MLflow</code>, Weights & Biases</p></li><li><p>Deployment: <code>Docker</code>, <code>Kubernetes</code>, <code>Seldon</code></p></li><li><p>Monitoring: <code>EvidentlyAI</code>, <code>Prometheus</code>, <code>Grafana</code></p></li></ul><p>üëâ If <code>ML</code> is about building models, <code>MLOps</code> is about keeping them alive and useful in production.</p><hr><h2 id=-case-study-mobile-teaching-ai-assistant-simplified>üì± Case Study: Mobile Teaching AI Assistant (Simplified)</h2><p>To connect theory with practice, let‚Äôs look at a simplified architecture for a Mobile Teaching AI Assistant ‚Äî a system designed to answer student questions, retrieve information, and provide context-aware explanations.</p><p><a href=mobile_banking_ai_assistant.png><img loading=lazy src=mobile_banking_ai_assistant.png alt="Mobile AI Assistant">
</a>{ data-lightbox=&ldquo;ai-post&rdquo; }</p><h3 id=-interaction-flow>üîÑ Interaction Flow</h3><ol><li><p>User Question ‚Äì A student asks a question via the mobile app.</p></li><li><p>App Backend ‚Äì The question is sent through a REST API to the AI backend.</p></li><li><p>Assistant Engine ‚Äì The engine processes the request and decides whether to answer directly or call an external API.</p></li><li><p>External AI Services ‚Äì Integration with providers like OpenAI, MS Azure, or translation APIs.</p></li><li><p>Response Delivery ‚Äì The final answer is sent back through the pipeline and displayed to the student in the mobile app.</p></li><li><p>Feedback Loop ‚Äì Students can provide feedback (e.g., was the answer helpful?), improving the system over time.</p></li></ol><h3 id=-architecture-layer>üèóÔ∏è Architecture Layer</h3><p><a href=mobile_banking_ai_assistant_arch.png><img loading=lazy src=mobile_banking_ai_assistant_arch.png alt="AI Assistant Architecture">
</a>{ data-lightbox=&ldquo;ai-post&rdquo; }</p><p>Behind the scenes, the assistant relies on a retrieval-augmented generation (RAG) pipeline:</p><ul><li><p>Sources ‚Äì PDFs, lecture notes, articles, and other documents.</p></li><li><p>Channels ‚Äì Ingestion pipelines that preprocess and clean the data.</p></li><li><p>Embeddings ‚Äì Text is transformed into vector embeddings using an embedding model.</p></li><li><p>Vector Store ‚Äì Stores embeddings for efficient semantic search.</p></li><li><p>Retriever + LLM ‚Äì A student‚Äôs question is embedded, compared against the vector store, and the top-ranked results are passed into an LLM (like GPT).</p></li><li><p>Ranked Results ‚Äì The LLM generates an answer that combines retrieved knowledge with generative reasoning.</p></li></ul><p>üëâ This setup ensures answers are relevant, context-aware, and explainable rather than ‚Äúhallucinated.‚Äù</p><h3 id=-why-it-matters-1>üåü Why It Matters</h3><p>This Mobile AI Assistant illustrates how the concepts from earlier sections (AI, ML, DL, and MLOps) come together:</p><ul><li><p><code>AI</code> provides the goal (a ‚Äúsmart‚Äù assistant).</p></li><li><p><code>ML/DL</code> powers embeddings and <code>LLM</code> reasoning.</p></li><li><p><code>MLOps</code> ensures the system is reliable, monitored, and retrainable.</p></li><li><p>Design, Develop, Deploy lifecycle is visible: from model design ‚Üí backend development ‚Üí mobile deployment.</p></li></ul><p>üìå This kind of system shows how abstract AI concepts translate into tangible software solutions that can impact education, healthcare, finance, and beyond.</p><hr><h2 id=-wrapping-up>üîÑ Wrapping Up</h2><ul><li><strong>AI</strong> = vision (smart systems)</li><li><strong>ML</strong> = method (learn from data)</li><li><strong>DL</strong> = breakthrough (neural nets at scale)</li></ul><p>Understanding these layers ‚Äî plus the risks, history, and market ‚Äî gives you the tools to cut through hype and apply AI effectively.</p><hr><p>üöÄ Follow me on <a href=https://norbix.dev>norbix.dev</a> for more insights on Go, Python, AI, system design, and engineering wisdom.</p></div><div class=post-subscribe><style>.subscribe-form{display:flex;gap:.5rem;margin-top:1rem;align-items:center}.subscribe-form input[type=email]{padding:.5rem;border-radius:6px;border:1px solid #ccc;background-color:#1f1f1f;color:#fff}.subscribe-form input[type=submit],.subscribe-form button{padding:.5rem 1rem;border-radius:6px;background-color:#facc15;color:#000;border:none;cursor:pointer;font-weight:700}.subscribe-form input[type=submit]:hover{background-color:#fcd34d}</style><form action=https://buttondown.email/api/emails/embed-subscribe/norbix method=post target=popupwindow onsubmit='window.open("https://buttondown.email/norbix","popupwindow")' class=subscribe-form><input type=email name=email placeholder="Enter your email" required>
<input type=submit value=Subscribe></form><p style=font-size:.875rem;opacity:.6>Powered by Buttondown.</p></div><div class=post-comments><script src=https://giscus.app/client.js data-repo=norbix/norbix.dev data-repo-id=R_kgDOOV_xMQ data-category=Announcements data-category-id=DIC_kwDOOV_xMc4CpF5M data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=dark data-lang=en crossorigin=anonymous async></script></div><nav class=paginav><a class=prev href=https://norbix.dev/posts/generic-protocol-pattern-in-go/><span class=title>¬´ Prev</span><br><span>The Generic Protocol Pattern in Go: Designing Extensible CLI Interfaces</span>
</a><a class=next href=https://norbix.dev/posts/architectural-patterns/><span class=title>Next ¬ª</span><br><span>Architectural Patterns in Go: MVC, Hexagonal, CQRS, and Microservices</span></a></nav></article></main></main><footer class=footer><span>&copy; 2025 <a href=https://norbix.dev/>norbix.dev - The log of my journey through code & software systems architecture</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script src=https://platform.linkedin.com/badges/js/profile.js async defer type=text/javascript></script></body></html>